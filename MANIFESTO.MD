# Why I’m Writing This

I’m sixteen years old, almost seventeen.
Until recently, my future felt pretty simple: study hard, get certifications, build cool things with code.
I was deep into cybersecurity, Python projects, and experimenting with AI tools. 
I had big dreams of launching startups, getting into a top university, and shaping a cyber-futuristic society.

Then I started learning about the real risks of AI.
Not just bias or job loss, but the possibility that the most powerful systems in human history could spin out of control.
Secret black box models. Hidden training data. Corporate secrecy. 
A reckless race to be first, even if it means cutting safety corners. 
All of it pointing toward a future that might not include us at all.

It hit me like a truck.
I went through what I can only call an existential crisis: panic, nihilism, apathy, all of it. 
And yes, I know it might sound dramatic or cringe for a teenager to say that, but it was real. 
It still is.

Now I’m left with one clear purpose: make AI safe, and make society ready for it, no matter what it takes.

## What I Believe

Transparency is survival.
Black box AI at the frontier level is a lethal gamble. If we can’t see inside, we can’t predict or control it.

Alignment comes before capability.
No new capability is worth pursuing if we can’t align it with human values first.

Public oversight is non negotiable.
A handful of companies should not decide the future of everyone on Earth behind closed doors.

Everyone deserves to know how AI makes decisions.
Not just the outputs, but the reasoning paths, the biases, and the limitations.

Technical and social safety must evolve together.
We need interpretability research, yes — but also laws, culture, and education to match it.

## What I’m Trying to Achieve

Interpretable AI — tools, methods, and research that make advanced models explainable to both experts and the public.

Open safety evaluations — independent audits before deployment, with results made public.

Global safety standards — enforceable rules that put alignment ahead of profit.

A culture of refusal — where developers, engineers, and researchers walk away from unsafe launches.

## What I’m Doing

I’m building:

    A public hub for AI transparency and interpretability research.

    Guides and tools so anyone can explore and understand model internals.

    A living roadmap for AI safety activism, from code to policy.

    Alliances with researchers, policy advocates, and ethical technologists.

## How You Can Join

This isn’t just my fight.

Whether you’re a student, a PhD, a policymaker, or just someone who cares about the future, you can:

    Contribute to open source interpretability tools.
    
    Translate and share these ideas globally.
    
    Pressure companies and regulators to demand transparency.
    
    Fund or support independent AI safety work.

## **My Promise**

I’m young, but I’m not naive.
I know this will take years and a lot of dedication.
I will not burn out or give up when it feels hopeless.
Every day, I’ll learn more, connect more, and push harder, because this is not about personal success anymore.

— Vilius Nikitinas

17-year-old who refuses to look away
