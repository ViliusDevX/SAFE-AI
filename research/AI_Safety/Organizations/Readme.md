# AI Safety Organizations

A collection of key organizations working on AI safety, AGI/ASI alignment, and governance.  
Each link goes directly to the official site, with a short note on their focus.

---

- [Center for AI Safety (CAIS)](https://safe.ai/) — Nonprofit advancing technical research and public awareness on AI risks.
- [Alignment Research Center (ARC)](https://alignment.org/) — Research nonprofit working on scalable oversight and alignment strategies.
- [Future of Life Institute (FLI)](https://futureoflife.org/) — NGO dedicated to reducing existential risks from advanced technologies, including AI.
- [Partnership on AI (PAI)](https://partnershiponai.org/) — Multi-stakeholder coalition developing best practices for responsible AI.
- [METR (Model Evaluation and Threat Research)](https://metr.org/) — Independent team evaluating capabilities and risks of frontier AI models.
- [Conscium](https://conscium.ai/) — UK-based company focusing on agent safety, verification, and neuromorphic approaches.
- [Machine Intelligence Research Institute (MIRI)](https://intelligence.org/) — Early research org focused on mathematical foundations of alignment.
- [Anthropic](https://www.anthropic.com/) — AI company with a strong focus on safety, alignment, and constitutional AI methods.
- [Center for Human-Compatible AI (CHAI)](https://humancompatible.ai/) — UC Berkeley research group led by Stuart Russell, working on value alignment.
- [Centre for the Governance of AI (GovAI)](https://governance.ai/) — Oxford-based org studying AI governance and policy for global coordination.
- [Redwood Research](https://www.redwoodresearch.org/) — Independent nonprofit doing applied alignment research and interpretability work.
- [OpenAI – Safety](https://openai.com/safety) — OpenAI’s page on ongoing alignment and safety research directions.
